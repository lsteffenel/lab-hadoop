{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce WordCount en Java\n",
    "\n",
    "Comme présenté dans le cours, Hadoop MapReduce a été initialement développé en Java et pour cette raison utilise préférentiellement du code source en Java. \n",
    "\n",
    "Dans ce premier exercice, vous allez apprendre à écrire et compiler un code pour compter les mots des livres de notre dataset.\n",
    "\n",
    "\n",
    "\n",
    "### Etape 1 : créer un \"Driver\"\n",
    "\n",
    "On appelle **driver** la classe qui est appellée en premier lors de l'exécution d'un programme. Dans notre cas, cette classe aura le rôle d'initialiser l'environnement Hadoop et de coordonner qui sera appelé à chaque étape (Map, Reduce). \n",
    "\n",
    "Dans le paragraphe suivant vous pouvez voir le code source pour un \"driver\". "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "package urca.bigdata;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        Configuration conf = new Configuration();\n",
    "        Job job = Job.getInstance(conf, \"word count\");\n",
    "        job.setJarByClass(WordCount.class);\n",
    "        job.setMapperClass(TokenizerMapper.class);\n",
    "        job.setCombinerClass(IntSumReducer.class);\n",
    "        job.setReducerClass(IntSumReducer.class);\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette classe (qui appartient au package \"urca.bigdata\") se connecte à l'environnement Hadoop en créant un `Job` \"**word count**\". Par la suite, elle renseigne certaines informations utiles au déroulement du programme : \n",
    "* quelle sera la classe utilisée pour le map --> `job.setMapperClass(TokenizerMapper.class);`\n",
    "* quelle sera la classe utilisée pour le reduce --> `job.setReducerClass(IntSumReducer.class);`\n",
    "* quelle sera la classe utilisée pour le combiner (optimisation locale) --> `job.setCombinerClass(IntSumReducer.class);`\n",
    "* quel est le format de la clé produite à la sortie (texte) --> `job.setOutputKeyClass(Text.class);`\n",
    "* quel est le format de la valeur produite à la sortie (entier) --> `job.setOutputValueClass(IntWritable.class);`\n",
    "* le point d'entrée pour la lecture des données à traiter (chemin spécifié dans le premier argument d'appel) --> `FileInputFormat.addInputPath(job, new Path(args[0]));`\n",
    "* l'endroi où les résultats seront enregistrés (deuxième argument d'appel) --> `FileOutputFormat.setOutputPath(job, new Path(args[1]));`\n",
    "\n",
    "On voit donc que ce **driver** fait appel à d'autres classes. Regardons tout d'abord la classe *TokenizerMapper.java* :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "package urca.bigdata;\n",
    "\n",
    "  import org.apache.hadoop.io.IntWritable;\n",
    "  import org.apache.hadoop.io.Text;\n",
    "  import org.apache.hadoop.mapreduce.Mapper;\n",
    "\n",
    "  import java.io.IOException;\n",
    "  import java.util.StringTokenizer;\n",
    "\n",
    "  public class TokenizerMapper\n",
    "        extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key, Text value, Mapper.Context context\n",
    "    ) throws IOException, InterruptedException {\n",
    "        StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "        while (itr.hasMoreTokens()) {\n",
    "            word.set(itr.nextToken());\n",
    "            context.write(word, one);\n",
    "        }\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette classe reçoit de Hadoop une ligne de texte avec une clé en format *Object*. Comme nous voulons compter les mots, la méthode `map()` effectue une *tokenisation*, c'est à dire, elle découpe la ligne en mots. \n",
    "\n",
    "Pour chaque mot produit, une paire **clé, valeur** sera produite (`context.write(word, one)`):\n",
    "* *word* contient le mot trouvé\n",
    "* *one* est un entier **1**\n",
    "\n",
    "L'ensemble des ces clés/valeurs passera d'abord par une optimisation locale (*combiner*) qui regroupera tous les mots identiques dans une machine, puis sera transmis au *Reducer* pour produire le comptage final. Comme l'opération combiner et reducer est la même (la seule chose qui change est l'étendu de l'opération) on utilise la même classe `IntSumReducer.java` :\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "package urca.bigdata;\n",
    "\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "\n",
    "import java.io.IOException;\n",
    "\n",
    "public class IntSumReducer\n",
    "        extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "\n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key, Iterable<IntWritable> values,\n",
    "                       Context context\n",
    "    ) throws IOException, InterruptedException {\n",
    "        int sum = 0;\n",
    "        for (IntWritable val : values) {\n",
    "            System.out.println(\"value: \"+val.get());\n",
    "            sum += val.get();\n",
    "        }\n",
    "        System.out.println(\"--> Sum = \"+sum);\n",
    "        result.set(sum);\n",
    "        context.write(key, result);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, la méthode `reduce()` reçoit des ensembles {clé, {liste de valeurs}}. En parcourant la liste, cette méthode récupère les valeurs et les additione, afin de produire une clé finale contenant **mot, somme**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiler et exécuter\n",
    "\n",
    "C'est le moment de mettre tout ensemble. \n",
    " Tout d'abord, vous allez compléter le code de chacun des fichiers avec le code indiqué ci-dessus. Pour cela, ouvrez chacun de ces fichiers à partir de la racine du dashboard (vous pouvez y retourner en cliquant sur le logo \"jupyterhub\" en haut à gauche), et copiez le code des paragraphes respectifs :\n",
    " \n",
    "* urca/bigdata/WordCount.java\n",
    "* urca/bigdata/TokenizerMapper.java\n",
    "* urca/bigdata/IntSumReducer.java\n",
    "\n",
    "\n",
    "### Compiler\n",
    "\n",
    "C'est bon, les fichiers sont remplis et sauvegardés ? C'est l'heure de les compiler et produire un **jar** pour Hadoop\n",
    "* si vous voulez, vous pouvez ouvrir un terminal à partir de la racine du dashboard (new->Terminal) et exécuter ces commandes vous même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mise en place d'un répertoire pour le code compilé\n",
    "! export HADOOP_CLASSPATH=$(hadoop classpath)\n",
    "! mkdir WordCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mise en place des variables d'environnement et compilation\n",
    "! javac -classpath `hadoop classpath`:. -d WordCount urca/bigdata/WordCount.java\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## création du jar\n",
    "! jar -cvf WC.jar -C WordCount/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exécution\n",
    "\n",
    "Voilà, tout est prêt. On a un Jar avec les exécutables qui Hadoop peut utiliser pour déployer l'application. On a le dataset dans HDFS. Il ne reste qu'à lancer le code.\n",
    "\n",
    "Pour cela, on va appeler Hadoop avec certains paramètres que vous devez pouvoir comprendre facilement : \n",
    "\n",
    "`hadoop jar WC.jar urca.bigdata.WordCount livres countLivres`\n",
    "\n",
    "On dit à hadoop de lancer le jar *WC.jar* et d'appeler la classe *urca.bigdata.WordCount*.\n",
    "Ensuite, on passe comme paramètres deux répertoires de HDFS :\n",
    "* le répertoire *livres* qui contient notre dataset\n",
    "* le répertoire *countLivres* qui contiendra le résultat\n",
    "\n",
    "### ATTENTION : le répertoire pour les résultats ne doit pas être crée avant : Hadoop se refuse d'écraser un répertoire existant \n",
    "\n",
    "Quand vous lancerez la commande dans le paragraphe ci-dessous, vous verez la progression du calcul. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop jar WC.jar urca.bigdata.WordCount livres countLivres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour finir, on peut regarder le résultat final stocké dans HDFS (et même le rappatrier) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls countLivres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat countLivres/part-r-00000 | tail -200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche une petite partie du résultat, mais vous pouvez observer qu'il y a encore du travail à faire pour rendre un résultat propre. Par exemple, on a des signes de ponctuation associés aux mots (*walk?*) qui comptent pour des mots différents. Le tokenizer pourrait filtrer ces signes afin de ne produire que des mots \"propres\".\n",
    "\n",
    "Cet exercice est fini, passons à l'activité 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
