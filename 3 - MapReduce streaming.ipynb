{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peut-on se passer de Java ?\n",
    "\n",
    "L'API de Hadoop MapReduce est en Java, ce qui permet d'explorer la totalité des possibilités de la plateforme. Toutefois, l'écriture du code Java est longue (plusieurs classes, des dizaines de lignes) et nécessite la compilation et la création d'un JAR.\n",
    "\n",
    "Si on n'a pas besoin de constructions trop élaborées (ex : on veut juste faire un map suivi d'un reduce), Hadoop propose une application *wrapper* permettant de faire appel à des scripts extérieurs pour effectuer les étapes de map et reduce. Ce *wrapper* fait partie de l'API Streaming.\n",
    "\n",
    "Ainsi, vous pouvez écrire des codes en Python, Bash, ou tout autre langage et les utiliser, du moment où ces codes acceptent des entrées **clé, valeur** et produisent des sorties **clé, valeur**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount en Python\n",
    "\n",
    "Nous allons donc refaire le code WordCount en utilisant deux codes Python, l'un pour le mapper, l'autre pour le reducer. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"mapper.py\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code lira les données de STDIN, les divisera en mots et produira une liste de **clé, valeur** vers STDOUT. Le script Map ne calculera pas une somme intermédiaire des occurrences d’un mot (pas de *combiner*). Au lieu de cela, il produira immédiatement des tuples {*word*, 1} même si un mot spécifique peut apparaître plusieurs fois dans l'entrée. Dans notre cas, nous laissons l'étape de réduction effectuer le décompte final."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce deuxième fichier lira les résultats de *mapper.py* à partir de STDIN et additionnera les occurrences de chaque mot à un décompte final, puis affichera ses résultats dans STDOUT.\n",
    "\n",
    "Attention : le format de sortie de *mapper.py* et le format d'entrée attendu de *reducer.py* doivent correspondre.\n",
    "\n",
    "Avant de lancer ce code sur Hadoop, on peut le tester directement sur un fichier texte pour voir si ça marche. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x mapper.py\n",
    "! chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat resources/datasets/livres/Charles_Darwin___On_the_Origin_of_Species_1st_Edition.txt | ./mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat resources/datasets/livres/Charles_Darwin___On_the_Origin_of_Species_1st_Edition.txt | ./mapper.py | sort -k1,1 | ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les tests montrent que les scripts fonctionnent. Il faut maintenant faire appel à Hadoop. La ligne de commande ci-dessous renseigne les différents éléments pour que Hadoop Streaming puisse trouver les scripts à exécuter ainsi que les fichiers source.\n",
    "* les entrées **-file** indiquent des fichiers qui doivent être copiés dans HDFS. En effet, le code à exécuter doit être accessible à toutes les machines d'un cluster.\n",
    "* les entrées **--mapper** et **--reducer** indiquent qui fait quoi\n",
    "* finalement, les entrées **-input** et **-output** indiquent le chemin HDFS des répertoires d'entrée et sortie des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop jar hadoop-*streaming*.jar \\\n",
    "-file ~/mapper.py    -mapper ~/mapper.py \\\n",
    "-file ~/reducer.py   -reducer ~/reducer.py \\\n",
    "-input livres -output gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat gutenberg-output/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupérer un fichier à partir de HDFS\n",
    "\n",
    "On a bien utilisé l'option `-put` pour mettre notre dataset dans HDFS. Souvent, le résultat d'un traitement (avec MapReduce, par exemple) sera aussi stocké dans HDFS. Dans ce cas, on peut le récupérer et copier dans notre \"disque local\" avec l'option `-get` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -get gutenberg-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, vous avez pu lancer un job mapreduce en Java et en Python. Il est maintenant le temps d'écrire votre propre code. Rendez-vous sur l'**activité 4 pour retrouver l'énoncé du projet à rendre**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
